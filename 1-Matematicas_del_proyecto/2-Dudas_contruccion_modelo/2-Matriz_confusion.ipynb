{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7842ab57",
   "metadata": {},
   "source": [
    "# Explicación de la Matriz de Confusión\n",
    "\n",
    "La **Matriz de Confusión** es una herramienta fundamental para evaluar el rendimiento de un modelo de clasificación, especialmente cuando la variable objetivo es categórica. Es una tabla que resume el rendimiento de un algoritmo de clasificación al mostrar el número de predicciones correctas e incorrectas hechas por el modelo, comparándolas con los valores reales (observados).\n",
    "\n",
    "Se utiliza comúnmente en problemas de clasificación binaria (dos clases), pero se puede extender a problemas de clasificación multiclase.\n",
    "\n",
    "## Estructura de una Matriz de Confusión (para Clasificación Binaria)\n",
    "\n",
    "Para un problema de clasificación binaria, donde tenemos una \"clase positiva\" (el evento que nos interesa predecir, ej. \"compró\", \"enfermo\") y una \"clase negativa\" (el evento opuesto, ej. \"no compró\", \"sano\"), la matriz de confusión se estructura de la siguiente manera:\n",
    "\n",
    "|                       | **Predicción: Positiva (1)** | **Predicción: Negativa (0)** |\n",
    "| :-------------------- | :--------------------------- | :--------------------------- |\n",
    "| **Real: Positiva (1)** | Verdaderos Positivos (VP)     | Falsos Negativos (FN)        |\n",
    "| **Real: Negativa (0)** | Falsos Positivos (FP)        | Verdaderos Negativos (VN)    |\n",
    "\n",
    "Donde cada celda representa un tipo de resultado de la predicción:\n",
    "\n",
    "* **Verdaderos Positivos (VP):** El modelo predijo correctamente la clase positiva cuando la clase real era positiva. (Aciertos de la clase positiva).\n",
    "* **Falsos Positivos (FP):** El modelo predijo incorrectamente la clase positiva cuando la clase real era negativa. (Errores de Tipo I o \"Falsa Alarma\").\n",
    "* **Falsos Negativos (FN):** El modelo predijo incorrectamente la clase negativa cuando la clase real era positiva. (Errores de Tipo II o \"Omisión\").\n",
    "* **Verdaderos Negativos (VN):** El modelo predijo correctamente la clase negativa cuando la clase real era negativa. (Aciertos de la clase negativa).\n",
    "\n",
    "## Métricas Derivadas de la Matriz de Confusión\n",
    "\n",
    "A partir de los valores de la matriz de confusión, se pueden calcular varias métricas clave para evaluar el rendimiento del modelo:\n",
    "\n",
    "1.  **Exactitud (Accuracy):** Mide la proporción de predicciones correctas sobre el total de predicciones. Es útil cuando las clases están balanceadas.\n",
    "\n",
    "    $$ \\text{Exactitud} = \\frac{\\text{VP} + \\text{VN}}{\\text{VP} + \\text{VN} + \\text{FP} + \\text{FN}} $$\n",
    "\n",
    "2.  **Precisión (Precision):** Mide la proporción de predicciones positivas correctas entre todas las predicciones positivas hechas por el modelo. Es relevante cuando el costo de un falso positivo es alto.\n",
    "\n",
    "    $$ \\text{Precisión} = \\frac{\\text{VP}}{\\text{VP} + \\text{FP}} $$\n",
    "\n",
    "3.  **Sensibilidad / Exhaustividad / Recall (Recall / Sensitivity):** Mide la proporción de casos positivos reales que fueron correctamente identificados por el modelo. Es importante cuando el costo de un falso negativo es alto.\n",
    "\n",
    "    $$ \\text{Sensibilidad} = \\frac{\\text{VP}}{\\text{VP} + \\text{FN}} $$\n",
    "\n",
    "4.  **Especificidad (Specificity):** Mide la proporción de casos negativos reales que fueron correctamente identificados por el modelo.\n",
    "\n",
    "    $$ \\text{Especificidad} = \\frac{\\text{VN}}{\\text{VN} + \\text{FP}} $$\n",
    "\n",
    "5.  **Puntuación F1 (F1-Score):** Es la media armónica de la Precisión y la Sensibilidad. Es útil cuando se busca un equilibrio entre ambas, especialmente en conjuntos de datos desequilibrados.\n",
    "\n",
    "    $$ \\text{F1-Score} = 2 \\times \\frac{\\text{Precisión} \\times \\text{Sensibilidad}}{\\text{Precisión} + \\text{Sensibilidad}} $$\n",
    "\n",
    "## Por qué es Importante la Matriz de Confusión\n",
    "\n",
    "* **Visión Detallada:** A diferencia de una métrica única como la Exactitud, la matriz de confusión proporciona una visión granular de dónde está fallando o acertando el modelo.\n",
    "* **Identificación de Errores Específicos:** Permite diferenciar entre Falsos Positivos y Falsos Negativos, lo cual es crucial cuando el costo asociado a cada tipo de error es diferente (ej. en diagnóstico médico, un Falso Negativo puede ser más grave que un Falso Positivo).\n",
    "* **Elección de Métricas Adecuadas:** Ayuda a decidir qué métricas de rendimiento son más relevantes para el problema específico que se está resolviendo. Por ejemplo, si predecir una enfermedad rara es el objetivo, la Sensibilidad es más importante que la Exactitud general.\n",
    "\n",
    "## Ejemplo de Uso\n",
    "\n",
    "Imagina un modelo que predice si un correo electrónico es spam (Positivo) o no spam (Negativo):\n",
    "\n",
    "|                   | **Predicción: Spam** | **Predicción: No Spam** |\n",
    "| :---------------- | :------------------- | :---------------------- |\n",
    "| **Real: Spam** | 90 (VP)              | 10 (FN)                 |\n",
    "| **Real: No Spam** | 5 (FP)               | 995 (VN)                |\n",
    "\n",
    "De esta matriz, podríamos calcular:\n",
    "* **Exactitud:** $(90+995) / (90+10+5+995) = 1085 / 1100 \\approx 0.986$ (Muy alta)\n",
    "* **Precisión (Spam):** $90 / (90+5) = 90 / 95 \\approx 0.947$ (De los correos que predijimos como spam, el 94.7% realmente lo eran).\n",
    "* **Sensibilidad (Spam):** $90 / (90+10) = 90 / 100 = 0.90$ (De todos los correos spam reales, el 90% fueron detectados).\n",
    "\n",
    "Este ejemplo ilustra cómo, incluso con una alta exactitud, analizar las otras métricas revela el rendimiento del modelo para cada clase y el tipo de errores que comete."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
